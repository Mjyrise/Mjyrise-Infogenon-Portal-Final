#!/usr/bin/env python3
# symbiotic_game_deploy.py - Advanced Automated Game Cloud Deployment

import os
import sys
import subprocess
import argparse
import base64
import json
import time
from pathlib import Path
from datetime import datetime

class GameCloudDeployer:
    def __init__(self):
        self.install_dir = Path("/opt/symbiotic_game")
        self.config = {
            'nodes': [],
            'ssh_key': Path.home() / ".ssh" / "game_cloud_key",
            'k3s_version': 'v1.26.5+k3s1',
            'ceph_version': 'quincy',
            'game_version': 'latest',
            'public_network': '10.0.0.0/24',
            'cluster_network': '10.1.0.0/24'
        }
        self.credentials = {}

    def run(self):
        self._parse_args()
        self._check_prerequisites()
        self._setup_ssh_key()
        self._prepare_nodes()
        self._gather_node_info()
        self._deploy_infrastructure()
        self._deploy_game()
        self._setup_monitoring()
        self._print_success()

    def _parse_args(self):
        parser = argparse.ArgumentParser(description='Automated Symbiotic Game Cloud Deployment')
        parser.add_argument('--nodes', nargs='+', required=True,
                          help="IP addresses of cluster nodes")
        parser.add_argument('--game-version', default='latest',
                          help="Version of the game to deploy")
        parser.add_argument('--public-net', default='10.0.0.0/24',
                          help="Public network CIDR for Ceph")
        parser.add_argument('--cluster-net', default='10.1.0.0/24',
                          help="Cluster network CIDR for Ceph")
        args = parser.parse_args()
        
        self.config['nodes'] = args.nodes
        self.config['game_version'] = args.game_version
        self.config['public_network'] = args.public_net
        self.config['cluster_network'] = args.cluster_net

    def _check_prerequisites(self):
        print("üîç Checking system prerequisites...")
        required = ['docker', 'curl', 'openssl', 'ssh', 'kubectl']
        missing = []
        
        for cmd in required:
            if not self._command_exists(cmd):
                missing.append(cmd)
                
        if missing:
            print(f"‚ùå Missing required commands: {', '.join(missing)}")
            print("Install missing packages with: sudo apt install docker.io curl openssl openssh-client kubectl")
            sys.exit(1)

    def _command_exists(self, cmd):
        return subprocess.call(['which', cmd], 
                             stdout=subprocess.DEVNULL,
                             stderr=subprocess.DEVNULL) == 0

    def _setup_ssh_key(self):
        if not self.config['ssh_key'].exists():
            print("üîë Generating SSH key for cluster access...")
            self.config['ssh_key'].parent.mkdir(exist_ok=True, parents=True)
            subprocess.run([
                'ssh-keygen', '-t', 'ed25519', '-N', '', '-f', str(self.config['ssh_key'])
            ], check=True)
            
        print("üîí Distributing SSH key to nodes...")
        for node in self.config['nodes']:
            try:
                subprocess.run([
                    'ssh-copy-id', '-i', str(self.config['ssh_key']), f"root@{node}"
                ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                print(f"  ‚úÖ Key deployed to {node}")
            except subprocess.CalledProcessError:
                print(f"  ‚ùå Failed to deploy key to {node}. Ensure root SSH access is enabled.")
                sys.exit(1)

    def _prepare_nodes(self):
        print("üõ† Preparing nodes (installing dependencies)...")
        for node in self.config['nodes']:
            print(f"  ‚öôÔ∏è Configuring {node}...")
            try:
                # Install required packages
                self._run_remote(node, "apt-get update && apt-get install -y docker.io curl ceph ceph-common ntp")
                
                # Configure Docker to use systemd cgroup driver
                self._run_remote(node, (
                    "mkdir -p /etc/docker && "
                    "echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' > /etc/docker/daemon.json && "
                    "systemctl restart docker"
                ))
                
                # Disable swap
                self._run_remote(node, "swapoff -a")
                self._run_remote(node, "sed -i '/swap/d' /etc/fstab")
                
                # Enable IP forwarding
                self._run_remote(node, (
                    "sysctl net.ipv4.ip_forward=1 && "
                    "echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf"
                ))
                
                print(f"  ‚úÖ Node {node} prepared successfully")
            except Exception as e:
                print(f"  ‚ùå Node preparation failed on {node}: {str(e)}")
                sys.exit(1)

    def _gather_node_info(self):
        print("üìä Gathering node information...")
        self.node_details = []
        
        for node in self.config['nodes']:
            try:
                cpu = self._run_remote(node, "nproc").strip()
                mem = self._run_remote(node, 
                    "free -h | grep Mem: | awk '{print $2}'").strip()
                disk = self._run_remote(node, 
                    "df -h --output=size / | tail -n 1").strip()
                kernel = self._run_remote(node, "uname -r").strip()
                os_info = self._run_remote(node, 
                    "grep PRETTY_NAME /etc/os-release | cut -d\\\" -f2").strip()
                
                self.node_details.append({
                    'ip': node,
                    'cpu': cpu,
                    'memory': mem,
                    'disk': disk,
                    'kernel': kernel,
                    'os': os_info
                })
            except Exception as e:
                print(f"  ‚ùå Failed to gather info from {node}: {str(e)}")
                sys.exit(1)
        
        # Print node summary
        print("\nüè¢ Cluster Node Summary:")
        for node in self.node_details:
            print(f"  - {node['ip']}: {node['os']}, {node['cpu']} CPU, {node['memory']} RAM, {node['disk']} Disk")

    def _deploy_infrastructure(self):
        print("üõ† Deploying core infrastructure...")
        
        # 1. Install k3s on first node (server)
        print("üöÄ Installing k3s control plane...")
        master_node = self.config['nodes'][0]
        try:
            k3s_install_cmd = f"""
            curl -sfL https://get.k3s.io | \
            INSTALL_K3S_VERSION='{self.config['k3s_version']}' \
            sh -s - server --cluster-init --disable traefik --node-taint CriticalAddonsOnly=true:NoExecute
            """
            self._run_remote(master_node, k3s_install_cmd)
            
            # Get kubeconfig
            kubeconfig = self._run_remote(master_node, "cat /etc/rancher/k3s/k3s.yaml")
            local_kube = Path.home() / ".kube" / "config"
            local_kube.parent.mkdir(exist_ok=True, parents=True)
            
            # Update server address in kubeconfig
            modified_kubeconfig = kubeconfig.replace("127.0.0.1", master_node)
            local_kube.write_text(modified_kubeconfig)
            
            # Set KUBECONFIG environment variable
            os.environ['KUBECONFIG'] = str(local_kube)
            
            print(f"  ‚úÖ k3s control plane installed on {master_node}")
        except Exception as e:
            print(f"  ‚ùå k3s installation failed: {str(e)}")
            sys.exit(1)
        
        # 2. Join worker nodes
        try:
            token = self._run_remote(master_node, "cat /var/lib/rancher/k3s/server/node-token").strip()
            for node in self.config['nodes'][1:]:
                print(f"  üõ† Adding worker node {node}...")
                join_cmd = f"""
                curl -sfL https://get.k3s.io | \
                K3S_URL=https://{master_node}:6443 \
                K3S_TOKEN={token} \
                sh -s - agent
                """
                self._run_remote(node, join_cmd)
                print(f"    ‚úÖ Node {node} joined cluster")
        except Exception as e:
            print(f"  ‚ùå Worker join failed: {str(e)}")
            sys.exit(1)
        
        # 3. Verify cluster status
        print("üîç Verifying cluster status...")
        time.sleep(10)  # Allow nodes to register
        try:
            nodes = subprocess.check_output(
                "kubectl get nodes -o wide", 
                shell=True, 
                stderr=subprocess.STDOUT
            ).decode()
            print(f"\nCluster Nodes:\n{nodes}")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Cluster verification failed: {e.output.decode()}")
            sys.exit(1)
        
        # 4. Install Ceph storage
        print("üíæ Setting up Ceph storage...")
        self._setup_ceph()

    def _setup_ceph(self):
        master_node = self.config['nodes'][0]
        try:
            print("  üîÑ Installing Ceph on all nodes...")
            for node in self.config['nodes']:
                self._run_remote(node, "apt-get install -y ceph ceph-common")
            
            print("  ‚öôÔ∏è Configuring Ceph cluster...")
            self._run_remote(master_node, "mkdir -p /etc/ceph")
            
            # Generate Ceph configuration
            mon_host = ",".join([f"{ip}:6789" for ip in self.config['nodes']])
            ceph_conf = f"""
            [global]
            fsid = {self._generate_uuid()}
            mon_initial_members = {','.join(self.config['nodes'])}
            mon_host = {mon_host}
            public_network = {self.config['public_network']}
            cluster_network = {self.config['cluster_network']}
            osd_pool_default_size = {len(self.config['nodes'])}
            auth_cluster_required = cephx
            auth_service_required = cephx
            auth_client_required = cephx
            """
            self._run_remote(master_node, f"echo '{ceph_conf}' > /etc/ceph/ceph.conf")
            
            # Initialize cluster
            print("  ‚ö° Bootstrapping Ceph cluster...")
            self._run_remote(master_node, "cephadm bootstrap --mon-ip $(hostname -I | awk '{print $1}')")
            
            # Add OSDs
            print("  ‚ûï Adding OSDs to cluster...")
            for node in self.config['nodes']:
                self._run_remote(node, f"ceph orch daemon add osd {node}:/var/lib/ceph/osd")
            
            print("  ‚úÖ Ceph storage configured successfully")
        except Exception as e:
            print(f"  ‚ùå Ceph setup failed: {str(e)}")
            sys.exit(1)

    def _deploy_game(self):
        print("üéÆ Deploying Symbiotic Game...")
        
        try:
            # Create namespace
            self._run_local("kubectl create namespace symbiotic-game")
            
            # Generate and store credentials
            db_password = self._generate_password(24)
            self.credentials['postgres'] = {
                'user': 'game',
                'password': db_password,
                'database': 'symbiotic'
            }
            
            # Create PostgreSQL secret
            secret_manifest = f"""
            apiVersion: v1
            kind: Secret
            metadata:
              name: postgres-creds
              namespace: symbiotic-game
            type: Opaque
            data:
              username: {base64.b64encode(b'game').decode()}
              password: {base64.b64encode(db_password.encode()).decode()}
            """
            self._apply_manifest(secret_manifest)
            
            # Deploy PostgreSQL
            pg_manifest = f"""
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: postgres
              namespace: symbiotic-game
            spec:
              serviceName: postgres
              replicas: 1
              selector:
                matchLabels:
                  app: postgres
              template:
                metadata:
                  labels:
                    app: postgres
                spec:
                  containers:
                  - name: postgres
                    image: postgres:13
                    env:
                    - name: POSTGRES_DB
                      value: symbiotic
                    - name: POSTGRES_USER
                      valueFrom:
                        secretKeyRef:
                          name: postgres-creds
                          key: username
                    - name: POSTGRES_PASSWORD
                      valueFrom:
                        secretKeyRef:
                          name: postgres-creds
                          key: password
                    ports:
                    - containerPort: 5432
                    volumeMounts:
                    - name: data
                      mountPath: /var/lib/postgresql/data
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  storageClassName: ceph-rbd
                  accessModes: [ "ReadWriteOnce" ]
                  resources:
                    requests:
                      storage: 100Gi
            """
            self._apply_manifest(pg_manifest)
            
            # Wait for PostgreSQL to initialize
            print("  ‚è≥ Waiting for PostgreSQL to become ready...")
            self._run_local("kubectl wait --namespace symbiotic-game --for=condition=ready pod -l app=postgres --timeout=300s")
            
            # Deploy Game Server
            game_manifest = f"""
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: game-server
              namespace: symbiotic-game
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: game
              template:
                metadata:
                  labels:
                    app: game
                spec:
                  containers:
                  - name: game
                    image: symbiotic-game:{self.config['game_version']}
                    ports:
                    - containerPort: 8000
                    env:
                    - name: DB_HOST
                      value: postgres.symbiotic-game.svc.cluster.local
                    - name: DB_USER
                      valueFrom:
                        secretKeyRef:
                          name: postgres-creds
                          key: username
                    - name: DB_PASSWORD
                      valueFrom:
                        secretKeyRef:
                          name: postgres-creds
                          key: password
                    - name: DB_NAME
                      value: symbiotic
                    resources:
                      requests:
                        cpu: "500m"
                        memory: "512Mi"
                      limits:
                        cpu: "2"
                        memory: "2Gi"
            """
            self._apply_manifest(game_manifest)
            
            # Expose service
            svc_manifest = """
            apiVersion: v1
            kind: Service
            metadata:
              name: game-service
              namespace: symbiotic-game
            spec:
              type: LoadBalancer
              selector:
                app: game
              ports:
              - protocol: TCP
                port: 80
                targetPort: 8000
            """
            self._apply_manifest(svc_manifest)
            
            print("  ‚úÖ Game deployment completed successfully")
        except Exception as e:
            print(f"  ‚ùå Game deployment failed: {str(e)}")
            sys.exit(1)

    def _setup_monitoring(self):
        print("üìä Setting up monitoring stack...")
        try:
            # Install Prometheus and Grafana
            self._run_local("helm repo add prometheus-community https://prometheus-community.github.io/helm-charts")
            self._run_local("helm repo update")
            self._run_local((
                "helm install prometheus prometheus-community/kube-prometheus-stack "
                "--namespace monitoring --create-namespace "
                "--set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false"
            ))
            
            # Expose Grafana
            self._run_local("kubectl patch svc prometheus-grafana -n monitoring -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'")
            
            print("  ‚úÖ Monitoring stack deployed")
        except Exception as e:
            print(f"  ‚ùå Monitoring setup failed: {str(e)}")

    def _apply_manifest(self, manifest):
        with open("/tmp/k8s_manifest.yaml", "w") as f:
            f.write(manifest.strip())
        self._run_local("kubectl apply -f /tmp/k8s_manifest.yaml")

    def _run_remote(self, node, command):
        result = subprocess.run([
            'ssh', '-i', str(self.config['ssh_key']), '-o', 'StrictHostKeyChecking=no',
            f"root@{node}", command
        ], capture_output=True, text=True, check=True)
        return result.stdout

    def _run_local(self, command):
        subprocess.run(command, shell=True, check=True)

    def _generate_password(self, length):
        return subprocess.check_output([
            'openssl', 'rand', '-base64', str(length)
        ]).decode('utf-8').strip()

    def _generate_uuid(self):
        return subprocess.check_output(['uuidgen']).decode('utf-8').strip()

    def _print_success(self):
        master_ip = self.config['nodes'][0]
        
        print("\n‚úÖ Symbiotic Game Cloud Deployment Complete!")
        print("="*60)
        print("üåê Access Information:")
        print(f"  Game URL: http://{master_ip}")
        print(f"  Grafana Dashboard: http://{master_ip}:3000")
        print("    Username: admin")
        print(f"    Password: {self._run_local('kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 --decode')}")
        print("\nüîê Database Credentials:")
        print(f"  Host: postgres.symbiotic-game.svc.cluster.local")
        print(f"  Database: symbiotic")
        print(f"  Username: game")
        print(f"  Password: {self.credentials['postgres']['password']}")
        
        print("\nüìã Cluster Management Commands:")
        print("  Scale game servers: kubectl scale deployment -n symbiotic-game game-server --replicas=5")
        print("  View game logs: kubectl logs -n symbiotic-game -l app=game --tail=100 -f")
        print("  Cluster status: kubectl get nodes,all -A")
        
        print("\nüí° Next Steps:")
        print("1. Configure DNS to point to your cluster IP")
        print("2. Set up automated backups with: ./symbiotic_game_deploy.py --backup")
        print("3. Monitor cluster health with Grafana dashboard")

if __name__ == "__main__":
    try:
        deployer = GameCloudDeployer()
        deployer.run()
    except KeyboardInterrupt:
        print("\nüö´ Deployment canceled by user")
        sys.exit(1)
